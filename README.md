# Prompt Benchmark <br>

A benchmarking platform for LLMs!<br>

This benchmarking platform indirectly calculates the META PROMPT GENERATION QUALITY of a model. Currently hosted at [Prompt Benchmark Console](https://prompt-benchmark-console-prod.redisland-25b20936.centralindia.azurecontainerapps.io/). Feel free to vote on the LLMs and help us make a better and a reliable leaderboard!<br>

## Running Locally 

To run the application locally, follow these steps:<br>

1. Open a terminal and start the FastAPI services first by using:
    ```bash
    python start.py
    ```

2. After the FastAPI services are up and running, start the Streamlit application in another terminal:
    ```bash
    python run_streamlit.py
    ```

## Contributing

We welcome contributions to improve the platform. Feel free to open issues or submit pull requests.<br>

---

Feel free to vote on the LLMs and help us make a better leaderboard!
