
# prompt-benchmark <br>

A bench marking platform for llms!!<br>
this bench marking platform indirectly caluclates the META PROMPT GENERATION QUALITY if a model <br>
currently hosted at the site : https://prompt-benchmark-console-prod.redisland-25b20936.centralindia.azurecontainerapps.io/ <br>
feel free to vote on the llms and help us make a better leaderboard!!! <br>


to run locally 
open one terminal and start the fast api services first by using 
python start.py

after its up and running start ther streamlit in another terminal python run_streamlit.py
