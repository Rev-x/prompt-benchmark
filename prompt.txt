i have a project with this type of structure 
project_root/
│
├── requirements.txt
│
├── server/
│   ├── __init__.py
│   ├── db.py
│   └── api_routes.py
│
├── client/
│   ├── app.py
│   ├── admin.py
│   └── leaderboard.py
│
└── .streamlit/
    └── config.toml
│
└── start.py
|
├──streamlit_app.py




__init__.py :

from .db import (
    add_or_update_prompt,
    add_or_update_use_case,
    add_or_update_assistant,
    fetch_models,
    fetch_use_cases,
    get_random_prompts,
    add_response,
    add_use_case,
    calculate_elo,
    update_elo,
    increment_total_games,
    db
)

db.py :

import os
from dotenv import load_dotenv
import json
import firebase_admin
from firebase_admin import credentials, firestore
import numpy as np

load_dotenv()

firebase_creds_json = os.getenv('FIREBASE_CREDENTIALS')

firebase_creds = json.loads(firebase_creds_json)

cred = credentials.Certificate(firebase_creds)
firebase_admin.initialize_app(cred)

db = firestore.client()


def get_random_prompts(use_case, n=2):
    prompts = db.collection('prompt').where('use_case', '==', use_case).stream()
    assistants = db.collection('assistants').where('use_case', '==', use_case).stream()
    
    prompts_list = [p.to_dict() for p in prompts]
    assistants_list = [a.to_dict() for a in assistants]

    latest_prompts = {}
    for prompt in prompts_list:
        origin = prompt.get('origin')
        if origin and (origin not in latest_prompts or latest_prompts[origin].get('version', 0) < prompt.get('version', 0)):
            latest_prompts[origin] = prompt

    latest_assistants = {}
    for assistant in assistants_list:
        origin = assistant.get('origin')
        if origin and (origin not in latest_assistants or latest_assistants[origin].get('version', 0) < assistant.get('version', 0)):
            latest_assistants[origin] = assistant

    combined_list = list(latest_prompts.values()) + list(latest_assistants.values())

    model_play_counts = {item.get('origin'): 0 for item in combined_list if 'origin' in item}
    total_games_ref = db.collection('total_games').document('total')
    total_games_doc = total_games_ref.get()

    total_games = total_games_doc.to_dict().get('total_games', 0) if total_games_doc.exists else 0

    for item in combined_list:
        origin = item.get('origin')
        if origin:
            model_ref = db.collection('elo').document(origin).get()
            if model_ref.exists:
                model_play_counts[origin] = model_ref.to_dict().get('no_of_games', 0)

    if total_games == 0:
        probabilities = {model: 1 for model in model_play_counts}
    else:
        probabilities = {model: 1 - (count / total_games) for model, count in model_play_counts.items()}

    models = list(probabilities.keys())
    weights = list(probabilities.values())
    
    if len(models) < n:
        # If not enough models are available, just pick random models
        selected_models = np.random.choice(models, size=n, replace=True)
    else:
        # Choose models based on the calculated probabilities
        selected_models = np.random.choice(models, size=n, replace=False, p=[w / sum(weights) for w in weights])

    selected_prompts = [item for item in combined_list if item.get('origin') in selected_models]
    result = []
    for item in selected_prompts:
        if 'assistant_id' in item:
            result.append({
                'assistant_id': item.get('assistant_id', ''),
                'assistant_apikey': item.get('assistant_apikey', ''),
                'origin': item.get('origin', ''),
                'use_case': item.get('use_case', ''),
                'assistant_version': item.get('assistant_version', ''),
                'version': item.get('version', '')
            })
        else:
            result.append({
                'origin': item.get('origin', ''),
                'prompt': item.get('prompt', ''),
                'use_case': item.get('use_case', ''),
                'version': item.get('version', '')
            })

    return result

def add_response(response_data):
    db.collection('responses').add(response_data)

def add_use_case(use_case_data):
    db.collection('use_cases').add(use_case_data)

def calculate_elo(current_rating, opponent_rating, score, k=30):
    expected_score = 1 / (1 + 10 ** ((opponent_rating - current_rating) / 400))
    new_rating = current_rating + k * (score - expected_score)
    return new_rating

def update_elo(model_a_name, model_b_name, result):
    try:
        model_a_ref = db.collection('elo').document(model_a_name)
        model_b_ref = db.collection('elo').document(model_b_name)

        model_a = model_a_ref.get()
        model_b = model_b_ref.get()

        initial_score = 1600

        if model_a.exists:
            model_a_data = model_a.to_dict()
            model_a_rating = model_a_data['score']
        else:
            model_a_rating = initial_score
            model_a_data = {
                'model_name': model_a_name,
                'no_of_games': 0,
                'score': model_a_rating
            }

        if model_b.exists:
            model_b_data = model_b.to_dict()
            model_b_rating = model_b_data['score']
        else:
            model_b_rating = initial_score
            model_b_data = {
                'model_name': model_b_name,
                'no_of_games': 0,
                'score': model_b_rating
            }

        if result == "win":
            model_a_new_rating = calculate_elo(model_a_rating, model_b_rating, 1)
            model_b_new_rating = calculate_elo(model_b_rating, model_a_rating, 0)
        elif result == "loss":
            model_a_new_rating = calculate_elo(model_a_rating, model_b_rating, 0)
            model_b_new_rating = calculate_elo(model_b_rating, model_a_rating, 1)
        elif result == "both_good":
            model_a_new_rating = calculate_elo(model_a_rating, model_b_rating, 0.75)
            model_b_new_rating = calculate_elo(model_b_rating, model_a_rating, 0.75)
        elif result == "both_bad":
            model_a_new_rating = calculate_elo(model_a_rating, model_b_rating, 0.25)
            model_b_new_rating = calculate_elo(model_b_rating, model_a_rating, 0.25)

        model_a_data['score'] = model_a_new_rating
        model_a_data['no_of_games'] += 1
        model_b_data['score'] = model_b_new_rating
        model_b_data['no_of_games'] += 1

        model_a_ref.set(model_a_data)
        model_b_ref.set(model_b_data)

        increment_total_games()

    except Exception as e:
        print(f"Error updating ELO: {e}")

def increment_total_games():
    total_games_ref = db.collection('total_games').document('total')
    total_games_doc = total_games_ref.get()

    if total_games_doc.exists:
        total_games = total_games_doc.to_dict().get('total_games', 0) + 1
    else:
        total_games = 1

    total_games_ref.set({'total_games': total_games}, merge=True)

def add_or_update_prompt(prompt_data):
    origin = prompt_data['origin']
    use_case = prompt_data['use_case']
    
    existing_prompts = db.collection('prompt').where('origin', '==', origin).where('use_case', '==', use_case).stream()
    versions = [p.to_dict()['version'] for p in existing_prompts]

    if versions:
        new_version = max(versions) + 1
    else:
        new_version = 1

    prompt_data['version'] = new_version
    db.collection('prompt').add(prompt_data)

def add_or_update_use_case(use_case_name):
    existing_use_cases = db.collection('use_cases').where('name', '==', use_case_name).stream()
    if not any(existing_use_cases):
        db.collection('use_cases').add({'name': use_case_name})

def add_or_update_assistant(assistant_data):
    use_case = assistant_data['use_case']
    
    # Fetch existing assistants for the use case
    existing_assistants = db.collection('assistants').where('use_case', '==', use_case).stream()
    versions = [a.to_dict()['version'] for a in existing_assistants]

    # Default to "Conva Assistant" and manage versioning
    if versions:
        new_version = max(versions) + 1
    else:
        new_version = 1

    assistant_data['origin'] = "Conva Assistant"
    assistant_data['version'] = new_version
    db.collection('assistants').add(assistant_data)

def fetch_models():
    models = set()
    prompts = db.collection('prompt').stream()
    for prompt in prompts:
        models.add(prompt.to_dict().get('origin', ''))
    return list(models)

def fetch_use_cases():
    use_cases = set()
    prompts = db.collection('use_cases').stream()
    for prompt in prompts:
        use_cases.add(prompt.to_dict().get('name', ''))
    return list(use_cases)

api_routes.py :

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Any
from . import db

router = APIRouter()

class Prompt(BaseModel):
    origin: str
    use_case: str
    prompt: str

class Assistant(BaseModel):
    assistant_id: str
    assistant_apikey: str
    assistant_version: str
    use_case: str

@router.get("/random_prompts/{use_case}")
async def get_random_prompts(use_case: str):
    return db.get_random_prompts(use_case)

@router.post("/add_response")
async def add_response(response_data: Dict[str, Any]):
    db.add_response(response_data)
    return {"message": "Response added successfully"}

@router.post("/update_elo")
async def update_elo(model_a: str, model_b: str, result: str):
    db.update_elo(model_a, model_b, result)
    return {"message": "ELO updated successfully"}

@router.post("/increment_total_games")
async def increment_total_games():
    db.increment_total_games()
    return {"message": "Total games incremented successfully"}

@router.get("/fetch_use_cases")
async def fetch_use_cases():
    return db.fetch_use_cases()

@router.get("/fetch_models")
async def fetch_models():
    return db.fetch_models()

@router.post("/add_prompt")
async def add_prompt(prompt: Prompt):
    db.add_or_update_prompt(prompt.dict())
    return {"message": "Prompt added successfully"}

@router.post("/add_use_case")
async def add_use_case(use_case: str):
    db.add_or_update_use_case(use_case)
    return {"message": "Use case added successfully"}

@router.post("/add_assistant")
async def add_assistant(assistant: Assistant):
    db.add_or_update_assistant(assistant.dict())
    return {"message": "Assistant added successfully"}

@router.get("/view_prompts/{model}/{use_case}")
async def view_prompts(model: str, use_case: str):
    prompts = db.db.collection('prompt').where('origin', '==', model).where('use_case', '==', use_case).stream()
    prompts_list = [p.to_dict() for p in prompts]
    return sorted(prompts_list, key=lambda x: x['version'], reverse=True)

@router.get("/view_assistants")
async def view_assistants():
    assistants = db.db.collection('assistants').stream()
    return [a.to_dict() for a in assistants]


@router.get("/leaderboard/{use_case}")
async def get_leaderboard(use_case: str):
    elo_docs = db.db.collection('elo').stream()
    leaderboard_data = []
    for doc in elo_docs:
        model_data = doc.to_dict()
        if 'use_case' not in model_data or model_data['use_case'] == use_case:
            leaderboard_data.append({
                'model_name': model_data['model_name'],
                'score': model_data['score'],
                'no_of_games': model_data['no_of_games']
            })
    return leaderboard_data

@router.get("/total_games")
async def get_total_games():
    total_games_doc = db.db.collection('total_games').document('total').get()
    if total_games_doc.exists:
        return total_games_doc.to_dict().get('total_games', 0)
    return 0

app.py :

import streamlit as st
import aiohttp
import asyncio
import nest_asyncio
import os
from openai import AsyncOpenAI
from conva_ai import AsyncConvaAI  
import time
import json

nest_asyncio.apply()

API_KEY = os.environ.get('OPENAI_API_KEY')
API_BASE_URL = "http://localhost:8000"  # Change this to your deployed API URL

client = AsyncOpenAI(api_key=API_KEY)

st.title("ELO Scoring Platform for LLMs")

async def fetch_api(endpoint, method="GET", data=None):
    async with aiohttp.ClientSession() as session:
        if method == "GET":
            async with session.get(f"{API_BASE_URL}{endpoint}") as response:
                return await response.json()
        elif method == "POST":
            async with session.post(f"{API_BASE_URL}{endpoint}", json=data) as response:
                return await response.json()

async def fetch_openai_response(prompt, model="gpt-4o-mini-2024-07-18"):
    response = await client.chat.completions.create(
        model=model,
        messages=[
            {"role": "user", "content": prompt}
        ],
        max_tokens=512
    )
    return response.choices[0].message.content

async def fetch_conva_ai_response(assistant_id, assistant_version, api_key, query):
    conva_client = AsyncConvaAI(
        assistant_id=assistant_id,
        assistant_version=assistant_version,
        api_key=api_key
    )
    response = await conva_client.invoke_capability(query, stream=False)
    response = response.model_dump()['parameters']
    response = json.dumps(response, indent=4)
    return response

async def handle_response(prompt, model_name, assistant_data=None):
    if model_name == 'Conva Assistant':
        assistant_id = assistant_data['assistant_id']
        assistant_version = str(assistant_data['assistant_version'])
        api_key = assistant_data['assistant_apikey']
        response = await fetch_conva_ai_response(assistant_id, assistant_version, api_key, prompt)
    else:
        response = await fetch_openai_response(prompt)
    return response

async def fetch_responses(user_input):
    prompts = await fetch_api(f"/random_prompts/{selected_option}")

    if prompts[0]['origin'] == 'Conva Assistant':
        assistant_data1 = prompts[0]
        prompt1 = user_input
    else:
        assistant_data1 = None
        prompt1 = prompts[0]['prompt'].replace("{query}", user_input)

    if prompts[1]['origin'] == 'Conva Assistant':
        assistant_data2 = prompts[1]
        prompt2 = user_input
    else:
        assistant_data2 = None
        prompt2 = prompts[1]['prompt'].replace("{query}", user_input)

    response1, response2 = await asyncio.gather(
        handle_response(prompt1, prompts[0]['origin'], assistant_data1),
        handle_response(prompt2, prompts[1]['origin'], assistant_data2)
    )

    st.session_state.prompt1 = prompt1
    st.session_state.prompt2 = prompt2
    st.session_state.llm1_response = response1
    st.session_state.llm2_response = response2
    st.session_state.model1_name = prompts[0]['origin']
    st.session_state.model2_name = prompts[1]['origin']

    st.session_state.show_results = False
    st.rerun()

options = asyncio.run(fetch_api("/fetch_use_cases"))
selected_option = st.selectbox("Select Use Case", options)

col1, col2 = st.columns(2)

if 'llm1_response' not in st.session_state:
    st.session_state.llm1_response = ""
if 'llm2_response' not in st.session_state:
    st.session_state.llm2_response = ""
if 'model1_name' not in st.session_state:
    st.session_state.model1_name = ""
if 'model2_name' not in st.session_state:
    st.session_state.model2_name = ""
if 'prompt1' not in st.session_state:
    st.session_state.prompt1 = ""
if 'prompt2' not in st.session_state:
    st.session_state.prompt2 = ""
if 'user_input' not in st.session_state:
    st.session_state.user_input = ""
if 'show_results' not in st.session_state:
    st.session_state.show_results = False
if 'winner' not in st.session_state:
    st.session_state.winner = ""

if not st.session_state.show_results:
    user_input = st.text_input("Enter your input here:", "")
    if st.button("Submit"):
        st.session_state.user_input = user_input
        asyncio.run(fetch_responses(user_input))

with col1:
    st.subheader("Response A")
    st.text_area("Model A", st.session_state.llm1_response, height=300, key="model_a")

with col2:
    st.subheader("Response B")
    st.text_area("Model B", st.session_state.llm2_response, height=300, key="model_b")

if st.session_state.llm1_response and st.session_state.llm2_response:
    st.subheader("Rate the responses")
    col4, col5, col6, col7 = st.columns(4)

    if col4.button("👈 Left (A)"):
        st.session_state.show_results = True
        st.session_state.winner = st.session_state.model1_name
        asyncio.run(fetch_api("/update_elo", method="POST", data={
            "model_a": st.session_state.model1_name,
            "model_b": st.session_state.model2_name,
            "result": "win"
        }))
        asyncio.run(fetch_api("/increment_total_games", method="POST"))
        st.rerun()
        
    if col5.button("Right (B) 👉"):
        st.session_state.show_results = True
        st.session_state.winner = st.session_state.model2_name
        asyncio.run(fetch_api("/update_elo", method="POST", data={
            "model_a": st.session_state.model1_name,
            "model_b": st.session_state.model2_name,
            "result": "loss"
        }))
        asyncio.run(fetch_api("/increment_total_games", method="POST"))
        st.rerun()

    if col6.button("Both Good"):
        st.session_state.show_results = True
        st.session_state.winner = "both_good"
        asyncio.run(fetch_api("/update_elo", method="POST", data={
            "model_a": st.session_state.model1_name,
            "model_b": st.session_state.model2_name,
            "result": "both_good"
        }))
        asyncio.run(fetch_api("/increment_total_games", method="POST"))
        st.rerun()
    
    if col7.button("Both Bad"):
        st.session_state.show_results = True
        st.session_state.winner = "both_bad"
        asyncio.run(fetch_api("/update_elo", method="POST", data={
            "model_a": st.session_state.model1_name,
            "model_b": st.session_state.model2_name,
            "result": "both_bad"
        }))
        asyncio.run(fetch_api("/increment_total_games", method="POST"))
        st.rerun()

if st.session_state.show_results:
    response_data = {
        'game_no': asyncio.run(fetch_api("/increment_total_games", method="POST"))['total_games'],
        'query': st.session_state.user_input,
        'use_case': selected_option,
        'model_a': st.session_state.model1_name,
        'model_b': st.session_state.model2_name,
        'response_a': st.session_state.llm1_response,
        'response_b': st.session_state.llm2_response,
        'winner_model': st.session_state.winner
    }
    asyncio.run(fetch_api("/add_response", method="POST", data=response_data))
    st.success("Response and rating added successfully!")

    with col1:
        st.write(f"Model A: {st.session_state.model1_name}")

    with col2:
        st.write(f"Model B: {st.session_state.model2_name}")

if st.button('reset'):
    for key in ['llm1_response', 'llm2_response', 'model1_name', 'model2_name', 'prompt1', 'prompt2', 'user_input', 'show_results', 'winner']:
        st.session_state[key] = ""
    st.rerun()

admin.py :

import streamlit as st
import pandas as pd
import aiohttp
import asyncio

API_BASE_URL = "http://localhost:8000"  # Change this to your deployed API URL

async def fetch_api(endpoint, method="GET", data=None):
    async with aiohttp.ClientSession() as session:
        if method == "GET":
            async with session.get(f"{API_BASE_URL}{endpoint}") as response:
                return await response.json()
        elif method == "POST":
            async with session.post(f"{API_BASE_URL}{endpoint}", json=data) as response:
                return await response.json()

st.title("Admin Panel")

tabs = st.tabs(["Add Prompt", "Add New Model", "View Prompts", "Manage Use Cases", "Add Assistant", "View Assistants"])

with tabs[0]:
    with st.form("Add Prompt Form"):
        models = asyncio.run(fetch_api("/fetch_models"))
        selected_model = st.selectbox("Select Model", models, key="model_select")

        use_cases = asyncio.run(fetch_api("/fetch_use_cases"))
        selected_use_case = st.selectbox("Select Use Case", use_cases, key="use_case_select")
        use_case = selected_use_case

        prompt_text = st.text_area("Prompt", key="prompt_text_area")

        if st.form_submit_button("Add Prompt"):
            if selected_model and use_case:
                asyncio.run(fetch_api("/add_prompt", method="POST", data={
                    "origin": selected_model,
                    "use_case": use_case,
                    "prompt": prompt_text
                }))
                st.success("Prompt added successfully with versioning!")
                st.session_state.prompt_text_area = ""
                st.rerun()
            else:
                st.error("Please select a valid model and use case.")

with tabs[1]:
    with st.form("Add New Model Form"):
        new_model = st.text_input("Enter New Model Name", key="new_model_input")

        use_cases = asyncio.run(fetch_api("/fetch_use_cases"))
        selected_use_case = st.selectbox("Select Use Case", use_cases, key="new_model_use_case_select")

        prompt_text = st.text_area("Prompt", key="new_model_prompt_text_area")

        if st.form_submit_button("Add New Model"):
            if new_model and selected_use_case:
                asyncio.run(fetch_api("/add_prompt", method="POST", data={
                    "origin": new_model,
                    "use_case": selected_use_case,
                    "prompt": prompt_text
                }))
                st.success("New model and prompt added successfully with versioning!")
                st.rerun()
            else:
                st.error("Please enter a valid model name and select a use case.")

with tabs[2]:
    models = asyncio.run(fetch_api("/fetch_models"))
    use_cases = asyncio.run(fetch_api("/fetch_use_cases"))
    selected_model = st.selectbox("Select Model to View Prompts", models, key="model_view")
    selected_use_case = st.selectbox("Select Use Case", use_cases, key="use_case_view")

    if selected_model and selected_use_case:
        prompts = asyncio.run(fetch_api(f"/view_prompts/{selected_model}/{selected_use_case}"))
        if prompts:
            df = pd.DataFrame(prompts)[['version', 'prompt']].set_index('version')
            st.dataframe(df, width=1200)
        else:
            st.write("No prompts found for the selected model and use case.")

with tabs[3]:
    with st.form("Add Use Case Form"):
        new_use_case = st.text_input("Enter New Use Case", key="new_use_case_input")

        if st.form_submit_button("Add Use Case"):
            if new_use_case:
                asyncio.run(fetch_api("/add_use_case", method="POST", data={"use_case": new_use_case}))
                st.success("New use case added successfully!")
                st.rerun()
            else:
                st.error("Please enter a valid use case.")

    use_cases = asyncio.run(fetch_api("/fetch_use_cases"))
    if use_cases:
        st.write("Existing Use Cases:")
        df = pd.DataFrame({'Use Case': use_cases})
        st.dataframe(df, width=1200)
    else:
        st.write("No use cases found.")

with tabs[4]:
    with st.form("Add Assistant Form"):
        assistant_id = st.text_input("Assistant ID", key="assistant_id")
        assistant_apikey = st.text_input("Assistant API Key", key="assistant_apikey")
        assistant_version = st.text_input("Assistant Version", key="assistant_version")
        use_cases = asyncio.run(fetch_api("/fetch_use_cases"))
        selected_use_case = st.selectbox("Select Use Case", use_cases, key="assistant_use_case")
        
        if st.form_submit_button("Add Assistant"):
            if assistant_id and assistant_apikey and assistant_version and selected_use_case:
                asyncio.run(fetch_api("/add_assistant", method="POST", data={
                    "assistant_id": assistant_id,
                    "assistant_apikey": assistant_apikey,
                    "assistant_version": assistant_version,
                    "use_case": selected_use_case
                }))
                st.success("Assistant information added successfully!")
                st.rerun()
            else:
                st.error("Please fill all fields.")

with tabs[5]:
    assistants = asyncio.run(fetch_api("/view_assistants"))
    if assistants:
        df = pd.DataFrame(assistants).set_index('assistant_id')
        st.dataframe(df, width=1200)
    else:
        st.write("No assistants found.")

leaderboard.py :

import streamlit as st
import pandas as pd
import aiohttp
import asyncio

API_BASE_URL = "http://localhost:8000"  # Change this to your deployed API URL

async def fetch_api(endpoint, method="GET", data=None):
    async with aiohttp.ClientSession() as session:
        if method == "GET":
            async with session.get(f"{API_BASE_URL}{endpoint}") as response:
                return await response.json()
        elif method == "POST":
            async with session.post(f"{API_BASE_URL}{endpoint}", json=data) as response:
                return await response.json()

st.title("ELO Scoring Leaderboard")

# Fetch use cases
use_cases = asyncio.run(fetch_api("/fetch_use_cases"))
selected_use_case = st.selectbox("Select Use Case", use_cases)

if selected_use_case:
    # Fetch leaderboard data
    leaderboard_data = asyncio.run(fetch_api(f"/leaderboard/{selected_use_case}"))
    
    if leaderboard_data:
        # Create a DataFrame from the leaderboard data
        df = pd.DataFrame(leaderboard_data)
        df = df.sort_values(by='score', ascending=False).reset_index(drop=True)
        df.index += 1  # Start index from 1
        
        # Display the leaderboard
        st.table(df)
    else:
        st.write("No data available for the selected use case.")

# Display total number of games played
total_games = asyncio.run(fetch_api("/total_games"))
st.write(f"Total games played: {total_games}")

# Option to download leaderboard as CSV
if st.button("Download Leaderboard as CSV"):
    csv = df.to_csv(index=False)
    st.download_button(
        label="Click here to download",
        data=csv,
        file_name=f"leaderboard_{selected_use_case}.csv",
        mime="text/csv",
    )

config.toml:
[server]
enableCORS = false
enableXsrfProtection = false

[theme]
primaryColor = "#F63366"
backgroundColor = "#FFFFFF"
secondaryBackgroundColor = "#F0F2F6"
textColor = "#262730"
font = "sans serif"


start.py:

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from server import api_routes
import streamlit as st
import streamlit.web.bootstrap as bootstrap
from streamlit.web.server import Server
import uvicorn
from starlette.applications import Starlette
from starlette.routing import Mount
import sys

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(api_routes.router)

def run_streamlit():
    sys.argv = ["streamlit", "run", "streamlit/app.py"]
    bootstrap.run(Server.get_current()._get_command_line(), "streamlit/app.py", "", [])

starlette_app = Starlette(routes=[
    Mount("/", app=app),
    Mount("/streamlit", app=run_streamlit)
])

if __name__ == "__main__":
    uvicorn.run(starlette_app, host="0.0.0.0", port=8000)

streamlit_app.py:

import streamlit as st
from streamlit_option_menu import option_menu
import client as client

def main():
    st.set_page_config(page_title="ELO Scoring Platform", layout="wide")

    with st.sidebar:
        selected = option_menu("Navigation", ["ELO Scoring Platform", "Admin Panel", "Leaderboard"],
                               icons=["house", "gear", "trophy"], menu_icon="cast", default_index=0)

    if selected == "ELO Scoring Platform":
        from client.app import main as app_main
        app_main()
    elif selected == "Admin Panel":
        from client.admin import main as admin_main
        admin_main()
    elif selected == "Leaderboard":
        from client.leaderboard import main as leaderboard_main
        leaderboard_main()

if __name__ == "__main__":
    main()



